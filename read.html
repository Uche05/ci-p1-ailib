<!DOCTYPE html>
<html lang="en">

<head>
    <title>Study AI</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!--Meta for SEO-->
    <meta name="description" content="AI Library by Uchechukwu Christian Kpadeuwa">
    <meta name="keywords" content="AI, AI Library, knowledge, history, current state of arts, ai">
    <!--Link to CSS-->
    <link rel="stylesheet" href="assets/css/style.css">
    <!--Favicon-->
    <link rel="icon" href="assets/images/robot.svg" type="image/svg+xml">
</head>

<body>
    <!--Header-->
    <header>
        <a href="index.html">
            <h1 id="header_logo">AILib</h1>
        </a>
        <!--Checkbox Toggle Menu-->
        <input type="checkbox" name="nav-tog" id="nav-tog">
            <i id="menu-img" class="fa-solid fa-bars"></i>
        </label>
        <!--Nav Bar-->
        <nav id="navbar">
            <ul id="menu">
                <li><a href="index.html">Home</a> </li>
                <li><a href="about.html">About AILib</a></li>
                <li><a href="read.html" class="active">Read</a></li>
                <li><a href="contact.html">Contact Us</a></li>
            </ul>
        </nav>
    </header>

    <!--Main Content-->
    <main>
        <section id="toc" aria-label="Table of Contents">
            <h2>AI Read ü§ì</h2>
            <hr>
            <ol>
                <li><a href="#c1">AI Intro</a></li>
                <li><a href="#c2">AI and its subsets</a></li>
                <li><a href="#c3">Goals for AI</a></li>
                <li><a href="#c4">History/Timeline of AI</a></li>
                <li><a href="#c5">ML</a></li>
                <li><a href="#c6">Understanding ChatGPT and LLMs</a></li>
                <li><a href="#c7">Ethics of AI</a></li>
                <li><a href="#c8">Fictious AI</a></li>
                <li><a href="#c9">AILib Blog</a></li>
            </ol>
        </section>

        <article id="c1" class="article-read">
            <h3>Brief Overview of AI.</h3>
            To the best of people's knowledge a computer does things it has been explicitly told to do or programmed to
            do. However, that is not the case with Artificial Intelligence, or AI. AI is the tech that allows computers
            perform tasks as a human would, without any explicit programming. It's simply the act of giving a computer a
            literal "brain". For this to be done, several other technologies and algorithms have to be implemented to
            actually achieve this goal, including: Machine Learning, Deep Learning, Natural Language Processing and
            others are otherwise called the subsets of Artificial Intelligence.

            <details>
                <summary>
                    <span>Automation vs. AI </span>
                </summary>
                <p>"Automation is the application of technology, programs, robotics or processes to achieve outcomes
                    with
                    minimal human input." [From <a href="https://www.ibm.com/topics/automation" target="_blank">here</a>
                    -IBM Learn 2024.
                    In other words, Automation refers to the use of technology to perform tasks without human
                    intervention.
                    This
                    does mean that automation does use predefined rules and programming to handle and perform tasks
                    efficiently
                    and repeatedly. Automation is mostly sought after to perform repetitive/overwhelming tasks. It is a
                    handy
                    tool in most digital workers' toolbelts. So, can automation implement AI to perform tasks? That
                    would be
                    a
                    game-changer such as the new Devon AI tool announced - it is possible to make automation more
                    AI-centric
                    as
                    it would actual use of intelligence rather reliance on pre-defined rules. </p>
            </details>

            <h4>Most commonly interchangeably used terms in the field of AI.</h4>

            <details>
                <summary>
                    <span>Algorithm vs. Model </span>
                </summary>
                <p> While technically different, these terms are often used interchangeably to describe the
                    mathematical process or structure used for AI tasks. An algorithm is the process or method, and a
                    model
                    is the output or result of that process, typically used in AI tasks like prediction, classification,
                    or
                    decision-making.
                </p>
            </details>
            <details>
                <summary>
                    <span>Machine Learning vs. Deep Learning </span>
                </summary>
                <p> Machine Learning involves algorithms that parse data, learn from that data, and then apply what
                    they have learned to make informed decisions while Deep Learning is a subset of ML that uses layered
                    neural networks to analyse various factors of data. Deep learning is particularly powerful for tasks
                    like image and speech recognition. </p>
            </details>
            <details>
                <summary>
                    <span>Neural Networks vs. Artificial Neural Networks(ANNs) </span>
                </summary>
                <p> Neural Networks generally refer to systems loosely modelled after the human ain that are designed
                    to recognize patterns.
                    Artificial Neural Networks are a specific type of neural network used in computing, which are key
                    components in deep learning designs.
                </p>
            </details>
            <details>
                <summary>
                    <span>Data Mining vs. Machine Learning </span>
                </summary>
                <p> Data Mining is the process of discovering patterns and knowledge from large amounts of data. The
                    process is exploratory.
                    Machine Learning is a method of data analysis that automates analytical model building. It uses
                    methods
                    from neural networks, statistics, operations research, and physics to find hidden insights in data
                    without explicitly being programmed for where to look or what to conclude.
                </p>
            </details>
            <details>
                <summary>
                    <span>Cognitive Computing vs. AI </span>
                </summary>
                <p> Cognitive Computing refers to systems that mimic human interaction by interpreting speech and
                    text, and responding in a human-like manner.
                    AI encompasses a wider array of technologies including those used in cognitive computing but also
                    includes systems that can perform tasks without necessarily mimicking human behaviour.
                </p>
            </details>
            <details>
                <summary>
                    <span>AI vs. Robotics </span>
                </summary>
                <p> AI involves creating algorithms and systems that can perform tasks that typically require human
                    intelligence.
                    Robotics is the branch of technology that deals with the design, construction, operation, and
                    application of robots. When robots are equipped with AI, they can perform tasks that involve a
                    degree of
                    autonomous decision-making. Robot and AI: While a robot is a physical machine, it often incorporates
                    AI
                    to perform tasks autonomously, blurring the lines between the two terms.
                </p>
            </details>
            <details>
                <summary>
                    <span>RPA vs. AI </span>
                </summary>
                <p> Robotic Process Automation (RPA) is a type of software automation that mimics actions that humans
                    might take, like logging into applications, entering data, completing tasks, and logging out. RPA is
                    typically rule-based and does not learn from its actions.
                    AI includes systems that can learn and adapt over time, improving their decision-making capabilities
                    based on experiences or newly acquired data.
                </p>
            </details>
            <details>
                <summary>
                    <span>BI vs. AI </span>
                </summary>
                <p> Business Intelligence typically refers to technologies, applications, and practices for the
                    collection, integration, analysis, and presentation of business information. The focus is on static
                    data
                    analysis to drive business decisions.
                    AI involves more dynamic analysis capabilities, including predictive analytics and machine learning,
                    where the system improves its analysis over time through learning.
                </p>
            </details>
            <details>
                <summary>
                    <span>Predictive Analytics vs. Machine Learning </span>
                </summary>
                <p> Predictive Analytics uses statistical models and forecasting techniques to understand the future.
                    It uses historical data to identify trends and determine how likely specific outcomes are in the
                    future.
                    Machine Learning is a subset of AI that involves teaching a computer to learn from data and make
                    decisions based on its learning, without being explicitly programmed. It is often used within
                    predictive
                    analytics but is broader in application and capability.
                </p>
            </details>
            <details>
                <summary>
                    <span>Expert Systems vs. AI </span>
                </summary>
                <p> Expert Systems are computer systems that emulate the decision-making ability of a human expert.
                    They use predefined rules and knowledge bases to make decisions, which means they don't learn or
                    adapt
                    beyond their initial programming.
                    AI encompasses a wider range of technologies, including those that can learn and adapt autonomously,
                    going beyond simple rule-based system
                </p>
            </details>
        </article>



        <article id="c2" class="article-read">
            <h3 id="ml">AI's subsets- ML</h3>
            Machine Learning (ML) is a powerful subset of Artificial Intelligence (AI) that focuses on developing
            algorithms and models that enable computers to learn from data and improve their performance on tasks over
            time, without being explicitly programmed for each task. The key idea is that the machine uses patterns and
            insights derived from data to make predictions, decisions, or recognize patterns. ML is where AI gets its
            capabilities from.

            <h4>Here are key concepts of Machine Learning</h4>

            <details>
                <summary>
                    <span>Supervised Learning </span>
                </summary>
                <p> This involves training a model on a labelled dataset, where the input data is paired with the
                    correct output. The model learns to predict the output from new, unseen inputs.
                </p>
            </details>
            <details>
                <summary>
                    <span>Unsupervised Learning </span>
                </summary>
                <p> The model learns from unlabelled data, identifying patterns, groupings, or structures within the
                    data without explicit instructions on what to predict.
                </p>
            </details>
            <details>
                <summary>
                    <span> Reinforcement Learning </span>
                </summary>
                <p>The model (often called an agent) learns by interacting with an environment and receiving feedback
                    in the form of rewards or penalties. The goal is to learn a strategy that maximizes the cumulative
                    reward.
                </p>
            </details>
            <details>
                <summary>
                    <span>Semi-supervised Learning </span>
                </summary>
                <p> This combines a small amount of labelled data with a large amount of unlabelled data. This
                    approach is used when labelling data is expensive or time-consuming.
                    For example; text classification, where only a few documents are labelled, but the model learns to
                    classify a large number of documents.
                </p>
            </details>
            <details>
                <summary>
                    <span>Transfer Learning </span>
                </summary>
                <p> Involves taking a pre-trained model developed for one task and applying it to a different but
                    related task. This approach leverages existing knowledge to improve performance on a new task.
                    For example: using a model trained on a large dataset of images to help classify medical images.
                </p>
            </details>
            <details>
                <summary>
                    <span>Deep Learning </span>
                </summary>
                <p> This is a very specialized subset of ML <a href="#ml">(Machine Learning)</a> that uses
                    multi-layered artificial neural networks (ANNs) to model complex patterns in large datasets. Deep
                    learning models excel at tasks like image and speech recognition.
                    Example: Convolutional Neural Networks (CNNs) for image classification or Recurrent Neural Networks
                    (RNNs) for sequence data like text or time series.
                </p>
            </details>
            <details>
                <summary>
                    <span>Artificial Neural Networks (ANNs)</span>
                </summary>
                <p>These are computational models inspired by the human ain, consisting of layers of interconnected
                    nodes (neurons). ANNs are the foundation of deep learning and can learn to approximate complex
                    functions
                    by adjusting the weights of the connections between neurons.
                    A simple ANN could be used for binary classification, while deeper, more complex ANNs are used in
                    deep
                    learning.
                </p>
            </details>

            <aside>
                <details>
                    <summary>
                        <h3>What can I call AI?</h3>
                    </summary>
                    <ol>
                        <li>Machine Learning Models: These include systems that learn from data to make predictions or
                            decisions without being explicitly programmed to perform the task. Examples include
                            predictive
                            models in finance, recommendation systems in e-commerce, and diagnostic tools in healthcare.
                            Natural Language Processing Systems: Technologies that process human language. This includes
                            everything from chatbots and virtual assistants like Siri and Alexa, to more complex systems
                            that
                            perform sentiment analysis, language translation, or content summarization.</li>
                        <li>Machines capable of undertaking tasks in the real world that are typically designed to
                            be executed by humans. When these robots are equipped with sensors and AI algorithms to
                            handle tasks
                            such as navigation, manipulation, and interaction, they are considered part of AI.</li>
                        <li>Expert Systems: These are AI systems that mimic the decision-making ability of a human
                            expert. By
                            processing a set of rules and logic, expert systems can make inferences and reach
                            conclusions. They
                            are used in various fields such as medicine (for diagnostic support), finance (for
                            investment
                            analysis), and more.</li>
                        <li>Vision Systems: These systems, which include face recognition and object recognition
                            technologies,
                            analyse and interpret the visual world. From security systems that identify individuals to
                            autonomous vehicles that "see" the road, these are quintessential AI applications.</li>
                        <li>Speech Recognition Systems: Systems that convert spoken words to text, often employed in
                            interactive voice response systems and virtual assistant devices. These systems understand
                            human
                            speech and are frequently used in devices that respond to voice commands.</li>
                        <li>Autonomous Vehicles: Cars, drones, or other vehicles that use AI to navigate safely without
                            human
                            input. These typically involve a combination of sensors, cameras, radar, and AI to make
                            real-time
                            decisions.</li>
                        <li>AI in Gaming: Advanced non-player characters (NPCs) in video games that adapt to player
                            actions
                            and strategies can be considered AI. These systems are designed to improve the gaming
                            experience by
                            providing responsive, challenging, and unpredictable interactions.</li>
                        <li>Predictive Maintenance Systems: Used in industrial settings, these systems analyse data from
                            equipment to predict failures before they happen. By understanding patterns and anomalies,
                            they can
                            pre-emptively suggest maintenance tasks.</li>
                        <li>Adaptive Learning Software: Educational software that adjusts content and assessments based
                            on a
                            student's particular needs. These systems analyse the student's performance and tailor the
                            educational experience to optimize learning outcomes.</li>
                    </ol>
                </details>
            </aside>
        </article>


        <article id="c3" class="article-read">
            <h3>Goals of AI</h3>
            <h4>"The various subfields of AI research are centred around particular goals and the use of particular
                tools. The traditional goals of AI research include&nbsp;reasoning,&nbsp;knowledge
                representation,&nbsp;planning,&nbsp;learning,&nbsp;natural language processing, perception, and support
                for robotics.
                ~From <a href="https://en.wikipedia.org/wiki/Artificial_intelligence" target="_blank">here</a>
            </h4>

            <details>
                <summary>
                    <span>Here are the goals of AI </span>
                </summary>
                <p>Reasoning and Problem-Solving: Develop algorithms that enable machines to reason logically, solve
                    puzzles,
                    and make decisions, incorporating methods to handle uncertainty and incomplete information.</p>
                <p>Knowledge Representation: Create systems that represent and utilize knowledge about the world,
                    enabling
                    AI to make informed decisions using knowledge bases and ontologies. This goal has been achieved by
                    certain AI models such as ChatGPT by OpenAI, Gemini, formally Google Bard by Google LLC and
                    Microsoft Co-pilot by Microsoft.</p>
                <p>Planning and Decision-Making: Equip AI agents with the ability to plan actions and make decisions
                    that
                    achieve goals or optimize outcomes, even under uncertain conditions.</p>
                <p>Learning: Design systems that can learn from data, improving their performance over time through
                    various
                    forms of machine learning, including supervised, unsupervised, and reinforcement learning.</p>
                <p>Natural Language Processing (NLP): Enable machines to understand, generate, and interact using human
                    language, tackling tasks like speech recognition, machine translation, and text generation.</p>
                <p>Perception: Develop AI that can perceive and interpret sensory inputs, such as vision, sound, and
                    touch,
                    to understand and interact with the physical world.</p>
                <p>Social Intelligence: Create AI systems that recognize and simulate human emotions, facilitating
                    socially intelligent interactions that are more engaging and human-like.</p>
                <p>General Intelligence: Pursue the long-term goal of developing Artificial General Intelligence (AGI),
                    capable of performing a wide range of tasks with the versatility and breadth of human intelligence.
                    In other words, the ability to complete any task performable by a human on an at least
                    equal level‚Äîis among the field's long-term goals.</p>
            </details>
        </article>


        <article id="c4" class="article-read">
            <div class="timeline-container">
                <div class="timeline">

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>Theory of computation</h3>
                            <p>1936</p>
                            <span class="tooltip">Alan Turing's theory suggested that machines could simulate any form
                                of
                                mathematical reasoning.</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>The Turing Test</h3>
                            <p> 1950</p>
                            <span class="tooltip"> Turing published "Computing Machinery and Intelligence,"</span>
                        </div>
                    </div>


                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>The term "AI" was born!</h3>
                            <p>1956</p>
                            <span class="tooltip">The Dartmouth Workshop marked the founding of AI as a field, and
                                "Artificial Intelligence" first coined in 1956 by John McCarthy.</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>The Perceptron</h3>
                            <p>1958</p>
                            <span class="tooltip">Frank Rosenblatt invented the perceptron, an algorithm for pattern
                                recognition based on a two-layer learning computer network to copy the human
                                brain.</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>AI Labs established</h3>
                            <p>1960s</p>
                            <span class="tooltip">AI programs show early success in solving problems like checkers,
                                algebra, and theorem proving. Hence AI Labs start in UK and US.</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>First AI Winter</h3>
                            <p>1969</p>
                            <span class="tooltip">Marvin Minsky and Seymour Papert published "Perceptrons," which showed
                                the limitations of perceptron models and led to first AI winter.</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>Second AI Winter</h3>
                            <p>1985</p>
                            <span class="tooltip"> AI market grows significantly, followed by the collapse of Lisp
                                Machines in 1987, leading to a second "AI Winter."</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>Neural Networks on the rise.</h3>
                            <p>1990s</p>
                            <span class="tooltip"> The rise of sub-symbolic approaches like neural networks, with Yann
                                LeCun demonstrating the success of convolutional neural networks (CNNs) in digit
                                recognition.
                                "</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>AI making progress fast.</h3>
                            <p>2000s</p>
                            <span class="tooltip"> AI starts producing verifiable results in specific applications,
                                leading to widespread adoption across industries.</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>Deep Learning dominating</h3>
                            <p>2012</p>
                            <span class="tooltip">Deep Learning starts dominating AI research and benchmarks, leading to
                                massive industry adoption.</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>AI making progress fast.</h3>
                            <p> 2015</p>
                            <span class="tooltip">AlphaGo by DeepMind defeats the world champion in Go, showcasing the
                                power of reinforcement learning and deep neural networks.</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>ChatGPT has been born!</h3>
                            <p>2020</p>
                            <span class="tooltip"> GPT-3 is released by OpenAI, demonstrating the capabilities of
                                large-scale language models in generating human-like text.</span>
                        </div>
                    </div>

                    <div class="timeline-event">
                        <div class="timeline-content">
                            <h3>The AI revolution!</h3>
                            <p>2022 - Present</p>
                            <span class="tooltip"> Massive investment in AI research and development, with billions of
                                dollars being invested annually and significant AI-related job openings.</span>
                        </div>
                    </div>

                    <!-- Add more events here -->
                </div>
            </div>

        </article>


        <article id="c5" class="article-read">
            <h3>Understanding ML</h3>
            <h4>Machine Learning algorithms</h4>
            <h5>Machine Learning (ML) algorithms are fundamental to the field of artificial intelligence, providing the
                means through which computers can learn from and make predictions or decisions based on data.
                <strong>
                    The choice of algorithm depends on the type of data available, the specific task at hand, and the
                    desired outcome.
                </strong>
            </h5>

            <details>
                <summary>
                    <span> How Machine Learning Algorithms Are Used</span>
                </summary>
                <p>- Feature Selection and Engineering: Before applying machine learning algorithms, significant effort
                    goes into
                    selecting the appropriate features (input variables) that make the most sense for the model to learn
                    effectively.
                    - Model Training: Algorithms learn from data by adjusting their parameters to minimize error between
                    their
                    predictions and the actual outcomes of the data.
                    - Model Evaluation: After training, models are evaluated using metrics like accuracy, precision,
                    recall, and F1
                    score for classification tasks, or mean squared error for regression tasks.
                    - Model Optimization: Techniques such as grid search or random search are used to find the optimal
                    settings for the
                    model‚Äôs parameters to improve performance.
                    - Prediction: Once trained and validated, models are used to predict outcomes on new, unseen data.
                </p>
            </details>

            <h4>Types of Machine Learning Algorithms </h4>

            <details>
                <summary>
                    <span>Supervised Machine Learning Models</span>
                </summary>
                <p>
                    These algorithms are trained using labelled data, i.e., data which has an input paired with the
                    correct output. The algorithm learns a model on this data to be able to predict the output
                    associated with new inputs.
                </p>
            </details>
            <details>
                <summary>
                    <span>Unsupervised Machine Learning Models</span>
                </summary>
                <p>
                    These algorithms are used when the data has no labels, and the goal is to infer the natural
                    structure present within a set of data points.
                </p>
            </details>
            <details>
                <summary>
                    <span>Semi-supervised Machine Learning Models</span>
                </summary>
                <p>
                    Combines a small amount of labelled data with a large amount of unlabelled data during training.
                    Semi-supervised learning is useful when the cost of labelling data is too high.
                </p>
            </details>
            <details>
                <summary>
                    <span>Reinforcement Machine Learning Models</span>
                </summary>
                <p>
                    In this type of ML, the algorithm learns to make specific decisions by trying to maximize a reward
                    signal. The learner is not told which actions to take but instead must discover which actions yield
                    the most reward by trying them.
                </p>
            </details>

            <h4>Examples of each mentioned type</h4>
            <details>
                <summary>
                    <span>Supervised Machine Learning Models</span>
                </summary>
                <ol>
                    <li>Linear Regression: Used for predicting a continuous value. For example, predicting house prices
                        based on features like area, age of the house, location, etc. </li>
                    <li>Logistic Regression: Used for binary classification tasks, such as spam detection or predicting
                        whether a customer will buy a product or not. </li>
                    <li>Decision Trees and Random Forests: These are used for classification and regression tasks. They
                        are particularly useful for medical diagnosis, customer segmentation, and financial analysis.
                    </li>
                </ol>
            </details>
            <details>
                <summary>
                    <span>Unsupervised Machine Learning Models</span>
                </summary>
                <ol>
                    <li>Clustering Algorithms (e.g., K-means, DBSCAN): Used to group a set of objects in such a way that
                        objects in the same group are more similar to each other than to those in other groups. Common
                        applications include customer segmentation, grouping experiment outcomes, and market
                        segmentation.</li>
                    <li>Principal Component Analysis (PCA): A dimensionality reduction technique used to reduce the
                        dimensionality of large data sets, increasing interpretability while minimizing information
                        loss. </li>
                </ol>
            </details>
            <details>
                <summary>
                    <span>Semi-supervised Machine Learning Models</span>
                </summary>
                <p>
                    It is used for tasks like image and video annotation where manual labelling is labour-intensive but
                    unlabelled data is plentiful.
                </p>
            </details>
            <details>
                <summary>
                    <span>Reinforcement Machine Learning Models</span>
                </summary>
                <p>
                    It is used for developing self-learning agents like those used in video games, autonomous vehicles,
                    or robot navigation systems.
                </p>
            </details>


        </article>


        <article id="c6" class="article-read">
            <h3>Understanding ChatGPT and other LLMs</h3>
            <h4>To be able to know as stated we'll explain how ChatGPT, a large language model developed by OpenAI based
                on the GPT (Generative Pre-trained Transformer) architecture's components/features are.</h4>

            <dl>
                <dt>It's Core</dt>
                <dd>ChatGPT is built on a neural network architecture known as a transformer(this concept was introduced
                    on a paper, ‚ÄúAttention is All You Need‚Äù by Vaswani et al. in 2017), which is particularly
                    effective for processing and generating natural language. They use self-attention mechanisms to
                    produce input data. It involves
                    weighing the importance of each word in a sentence, regardless of their position to enable it
                    understand the context and relationships between words in a sentence.</dd>
            </dl>

            <dl>
                <dt>The Training Process</dt>
                <dd>This consists of <strong>pre-training</strong> where GPT models undergo a phase of pre-training
                    where they learn from
                    a vast tanks of text data using unsupervised learning as GPT models are typically trained using a
                    variant of language modelling, specifically predicting the next word in a sequence given the
                    previous
                    words (autoregressive language modelling). This helps the model learn a broad understanding of
                    language, including grammar, facts, and various language styles and semantics.
                    After pre-training, the model can be fine-tuned on smaller, task-specific datasets.
                    This stage tailors the model‚Äôs general capabilities to particular applications such as answering
                    questions, translating text, or generating content. Supervised Learning is used in
                    <strong>fine-tuning</strong>,
                    the model might use labelled data specific to a task, adjusting its parameters to minimize error
                    and optimize performance for that task.
                </dd>
            </dl>

            <dl>
                <dt>Structural components</dt>
                <dd>- Multi-Head Attention (Layers and heads): Each layer has multiple 'heads' in its attention
                    mechanisms, allowing the model to simultaneously focus on different parts of the input sequence for
                    better context understanding.
                    - Feed-Forward Neural Networks: Each layer also includes feed-forward neural networks that process
                    the outputs from the attention mechanisms.
                    - Normalization and Residual Connections: These are used in each layer to help stabilize the
                    learning process and improve the flow of gradients through the network during training.
                    - Input Embeddings: Words are converted into numerical form to be processed by the model. Each word
                    is represented by a unique vector in a high-dimensional space.
                    - Positional Encoding: Since transformers do not inherently process the order of the input (unlike
                    RNNs), positional encodings are added to give the model information about the position of words in
                    the input sequence.


                </dd>
            </dl>

            <dl>
                <dt>Decoding strategies for text generation</dt>
                <dd>When generating text, GPT uses various strategies to decide which word to produce next:
                    - Greedy Decoding: Always picks the most likely next word.
                    - Beam Search: Considers multiple possible sequences simultaneously, choosing the sequence that has
                    the overall
                    highest probability.
                    - Top-K Sampling: Randomly picks the next word from the top K likely candidates, balancing
                    creativity and accuracy.
                </dd>
            </dl>
        </article>

        <article id="c7" class="article-read">

            <details>
                <summary>
                    <h3>Ethics with AI</h3>
                </summary>
                <p>
                    As Artificial Intelligence (AI) continues to permeate various aspects of society, the ethical
                    implications of AI technologies have become a major concern. Ethical AI development involves
                    ensuring that AI systems are designed and deployed in ways that are fair, transparent, and
                    beneficial to humanity. This note explores the key principles of AI ethics, reviews legal
                    guidelines, and provides advice for developers involved in creating AI-based products.
                </p>
                <ol>
                    <li>
                        <dl>
                            <dt>Fairness and Non-Discrimination</dt>
                            <dd>AI systems should be designed to treat all individuals fairly, without discrimination
                                based on race, gender, age, or any other protected characteristic. Developers must be
                                aware of and mitigate biases in data and algorithms to prevent perpetuating societal
                                inequalities. </dd>
                        </dl>
                    </li>
                    <li>
                        <dl>
                            <dt>Transparency</dt>
                            <dd>AI models should be transparent and their decision-making processes should be
                                understandable to users. It is crucial, especially in high-stakes areas like healthcare
                                or criminal justice, where decisions significantly impact individuals' lives. </dd>
                        </dl>
                    </li>
                    <li>
                        <dl>
                            <dt>Privacy and Data Protection</dt>
                            <dd>AI systems often rely on large datasets, including personal information. It is essential
                                to uphold individuals' privacy rights by ensuring that data is collected, stored, and
                                processed securely and with consent. Developers should adhere to data protection laws,
                                such as the General Data Protection Regulation (GDPR) in Europe.</dd>
                        </dl>
                    </li>
                    <li>
                        <dl>
                            <dt>Accountability</dt>
                            <dd>Developers, companies, and organizations deploying AI systems must be accountable for
                                their actions and the outcomes of their AI products. There should be mechanisms in place
                                to identify and address harmful consequences or errors in AI systems. </dd>
                        </dl>
                    </li>
                    <li>
                        <dl>
                            <dt>Safety and Security</dt>
                            <dd>AI systems must be safe and secure, ensuring that they do not pose risks to users or
                                society at large. This includes protection against hacking, unauthorized access, and
                                other forms of malicious interference. </dd>
                        </dl>
                    </li>
                    <li>
                        <dl>
                            <dt>Human-Centric Design</dt>
                            <dd>AI should enhance human capabilities rather than replace them. Developers should
                                prioritize user well-being, ensuring that AI systems empower individuals and do not lead
                                to dehumanization or reduced human agency.</dd>
                        </dl>
                    </li>
                    <li>
                        <dl>
                            <dt>Environmental Sustainability</dt>
                            <dd>The environmental impact of AI, including energy consumption and resource use, should be
                                considered in the development process. Developers are encouraged to design AI systems
                                that are energy-efficient and contribute to sustainable practices. </dd>
                        </dl>
                    </li>
                </ol>
            </details>

            <h4>Legal AI Ethic Guidelines.</h4>
            <ol>
                <li>The European Commission‚Äôs Ethics Guidelines for Trustworthy AI</li>
                <li>The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems</li>
                <li>The AI Ethics Guidelines by OECD</li>
            </ol>

            <em>
                AI ethics is not just a set of guidelines but a commitment to responsible innovation. By integrating
                ethical considerations into the development process, AI developers can create systems that are not only
                effective but also fair, transparent, and aligned with human values. This approach helps build trust in
                AI technologies, ensuring that they contribute positively to society.
            </em>
        </article>


        <article id="c8" class="article-read">


            <details>
                <summary>
                    <h3>AI and Fiction</h3>
                </summary>
                <p class="p-fictai">
                    Greek myths of Hephaestus and Pygmalion incorporated the idea of intelligent automata (such as
                    Talos) and artificial beings (such as Galatea and Pandora).
                    <br>
                    Hero of Alexandria created mechanical men and other automatons. He produced what may have been
                    <cite>"the world's first practical programmable machine:" </cite> an automatic theatre.
                    <br>
                    Dystopian Movies about automated humanoids that can act and make decisions independently like
                    <cite>Atlas
                        2024.</cite>
                </p>
            </details>

        </article>


        <article id="c9" class="article-read">
            <h3>AILib and our blog on AI.</h3>

            <details>
                <summary>
                    <span> Advice for AI Developers </span>
                </summary>

                <p>1. Embed Ethics from the Start:
                    Ethics should be integrated into every stage of AI development, from conceptualization and design to
                    deployment and maintenance. Consider the potential ethical implications of your AI system before
                    writing the first line of code.</p>

                <p>2. Perform Regular Audits and Bias Testing:
                    Regularly audit AI systems for fairness, accuracy, and bias. Use diverse datasets and conduct tests
                    to ensure that your system does not disproportionately disadvantage any group.</p>

                <p>3. Prioritize User Consent and Data Privacy:
                    Always obtain informed consent from users before collecting their data. Implement strong data
                    encryption and anonymization techniques to protect user privacy.</p>

                <p>4. Design for Transparency:
                    Make your AI models as transparent as possible. Provide users with explanations of how decisions are
                    made and ensure that your system‚Äôs operations are understandable, especially when the AI impacts
                    critical decisions.</p>

                <p>5. Ensure Robustness and Security:
                    Build AI systems that are robust and resilient to attacks or failures. Implement security measures
                    to protect the system from hacking and unauthorized use.</p>

                <p>6. Foster a Collaborative Ethical Culture:
                    Encourage open discussions about ethics within your team and organization. Foster a culture where
                    ethical concerns can be raised without fear of retribution, and where continuous learning about AI
                    ethics is encouraged.</p>

                <p>7. Engage with Stakeholders:
                    Engage with a diverse group of stakeholders, including ethicists, legal experts, users, and affected
                    communities, to gather input on potential ethical issues. This helps ensure that the AI system
                    aligns with broader societal values.</p>

                <p>8. Prepare for Accountability:
                    Establish clear lines of accountability for AI systems. Have processes in place to address and
                    rectify any negative impacts or errors that arise from the AI‚Äôs deployment.</p>

                <p>9. Stay Informed on Legal and Ethical Standards:
                    AI ethics is a rapidly evolving field. Stay informed about the latest legal requirements, ethical
                    standards, and best practices in AI development. Engage with the broader AI ethics community to keep
                    up with emerging issues.</p>
            </details>
        </article>


    </main>

    <!--Footer-->
    <footer id="read-footer">
        <p id="social-p">Find us on our socials!</p>
        <div>
            <ul id="socials">
                <li><a href="http://www.facebook.com" target="_blank" class="icon"
                        aria-label="Go to our facebook page (opens in new tab)"><i class="fa-brands fa-facebook"></i>
                    </a></li>
                <li><a href="http://www.instagram.com" target="_blank" class="icon"
                        aria-label="Go to our instagram page (opens in new tab)"><i class="fa-brands fa-instagram"></i>
                    </a></li>
                <li><a href="http://www.twitter.com" target="_blank" class="icon"
                        aria-label="Go to our twitter page (opens in new tab)"><i class="fa-brands fa-x-twitter"></i>
                    </a></li>
                <li><a href="https://www.linkedin.com/in/christian-k-399bab215" target="_blank" class="icon"
                        aria-label="Go to our linkedin page (opens in new tab)"><i class="fa-brands fa-linkedin"></i>
                    </a></li>
            </ul>
        </div>


        <p>¬© 2024 AILib. All Rights Reserved</p>

    </footer>
    <!--Script after the footer-->
    <script src="https://kit.fontawesome.com/af753a5c1b.js" crossorigin="anonymous" style="display: none;"></script>




</body>

</html>